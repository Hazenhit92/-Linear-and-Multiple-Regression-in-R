---
title: "Chapter 3- Regression Prediction"
output:
  html_document: default
  pdf_document: default
---
## Simple Regression Model
#### Using the Amusement park data provided in the Book
```{r}
df<-read.csv("http://goo.gl/HKnl74")
head(df)
```

```{r}
dim(df)
```
```{r}
library(gpairs)
gpairs(df) #Graphical Summary
```

```{r}
#distance is skwed, so we can apply log transform for it
df$logdistance<- log(df$distance)
hist(df$logdistance)
```

```{r}
head(df)
```

```{r}
library(corrplot)
corrplot.mixed(cor(df[,c(2,4:9)]), upper="ellipse")
```
## Regression with single independent variable
#### Let's consider the variable to be rides
```{r}
reg=lm(overall~rides, data=df)
summary(reg)
```

```{r}
plot(overall~rides,data=df, xlab="Number of Rides", ylab= "Overall Satisfaction")
reg=lm(overall~rides, data=df)
abline(reg)
```

```{r}
plot(reg$fitted.values,reg$residuals) #residual plot
```
## Prediction`

```{r}
names(reg)
```

```{r}
coef(reg)
```

```{r}
confint(reg)
```

```{r}
predict(reg, data.frame(rides=c(95)), interval="confidence")
predict(reg, data.frame(rides=c(95)), interval="prediction")
```

#### The 95% confidence interval associated with a ride value of 95 is (64.63,,69.05), and the  95% prediction interval is (41.44,92.25)

## Muutiple Linear Regression

#### Data Normalization
```{r}
df_std<-df[,-3] #Drop the distance column
dim(df_std)
head(df_std)
```

```{r}
df_std[,3:8]<- scale(df_std[,3:8]) # Data Normalization
head(df_std)
```

```{r}
m_reg<- lm(overall~ rides+ games+ wait+ clean+ weekend+ logdistance+ num.child, data= df_std)
summary(m_reg)
```
#### One of the most important uses of factors is in statistical modeling; since categorical variables enter into statistical models differently than continuous variables, storing data as factors insures that the modeling functions will treat such data correctly.
```{r}
factor(df_std$num.child)
#factor as an Independent Variable
df_std$num.child.factor<- factor(df_std$num.child)
```

```{r}
df_std[1:5,] #This is same as head(df_std)
```

```{r}
m_reg1<- lm(overall~ rides+ games+ wait+ clean+ weekend+ logdistance+ num.child.factor, data= df_std)
summary(m_reg1)
```

```{r}
AIC(m_reg); AIC(m_reg1)  #information criteria
BIC(m_reg); BIC(m_reg1)
```

#### When comparing models fitted by maximum likelihood to the same data, the smaller the AIC or BIC, the better the fit.

```{r}
# Model with binary has.child variable
df_std$has.child<- factor(df_std$num.child>0)
head(df_std)
```

```{r}
m_reg2<- lm(overall~ rides+ games+ wait+ clean+ weekend+ logdistance+ has.child, data= df_std)
summary(m_reg2)
```

```{r}
AIC(m_reg); AIC(m_reg1); AIC(m_reg2)   #information criteria
BIC(m_reg); BIC(m_reg1); BIC(m_reg2)

```
#### Combining factors to binary variable gave better AIC and BIC. This is a type of model selection procedure.

## Bayesian Linear Models
```{r}
library(MCMCpack)
m_reg3<- MCMCregress(overall~rides + games + wait + clean + weekend + 
    logdistance + has.child, data = df_std)
summary(m_reg3)
```

